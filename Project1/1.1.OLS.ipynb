{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c624121f-7057-4e48-8d21-d7a4257cbdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plain OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7465b20-2201-4fae-be2e-f198e7937902",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def MSE(y_data,y_model):\n",
    "    n = np.size(y_model)\n",
    "    return np.sum((y_data-y_model)**2)/n\n",
    "\n",
    "def R2(y_data, y_model):\n",
    "    return 1 - np.sum((y_data - y_model) ** 2) / np.sum((y_data - np.mean(y_data)) ** 2)\n",
    "\n",
    "#varying the grid size:\n",
    "for n in [200, 500, 1000]:    \n",
    "\n",
    "    #maximum polynomial degree+1\n",
    "    maxdegree = 9\n",
    "\n",
    "    # Make data set: the longtitude values of the Northern Hemisphere in radiants.\n",
    "    x = np.linspace(0,(np.pi)/2,n).reshape(-1, 1)\n",
    "\n",
    "    #Northern H. parameters\n",
    "    s0 = 1\n",
    "    s2 = -0.473\n",
    "    a0 = 0.675\n",
    "    a2 = -0.192\n",
    "    i2 = -0.165\n",
    "\n",
    "    #flux function (eqn. (14) from Stone_1978)\n",
    "    y = 0.5*(s0*a2+s2*a0+(2/7)*s2*a2-i2)*((np.sin(x))**3-np.sin(x))\n",
    "\n",
    "    #noisy flux function\n",
    "    y_noisy = np.random.normal(y, abs(y*0.05)) \n",
    "\n",
    "    #varying the test/training data percentage\n",
    "    for test_size in [0.25, 0.5, 0.75]:\n",
    "        print('number of datapoints:', n,', test data percentage:', 100*test_size,'%')\n",
    "        \n",
    "        #MSE OLS analysis\n",
    "        MSE_train = np.zeros(maxdegree)\n",
    "        MSE_test = np.zeros(maxdegree)\n",
    "\n",
    "        #Correlation OLS analysis\n",
    "        R2_train=np.zeros(maxdegree)\n",
    "        R2_test=np.zeros(maxdegree)\n",
    "\n",
    "        polydegree = np.zeros(maxdegree) \n",
    "\n",
    "        for degree in range(maxdegree):\n",
    "            #polynomial fit\n",
    "            polydegree[degree]=degree\n",
    "            poly = PolynomialFeatures(degree=degree)\n",
    "            X = poly.fit_transform(x)\n",
    "            \n",
    "            n_iter=100\n",
    "            \n",
    "            MSE_train_iter=np.zeros(n_iter)\n",
    "            MSE_test_iter=np.zeros(n_iter)\n",
    "            \n",
    "            R2_train_iter=np.zeros(n_iter)\n",
    "            R2_test_iter=np.zeros(n_iter)\n",
    "            \n",
    "            #we iterrate 100 times to avoid erratic behvious. At least while the noise is turned off...\n",
    "            for iter in range (n_iter):\n",
    "\n",
    "                #splitting of the data\n",
    "                X_train, X_test, y_train, y_test = train_test_split(X, y_noisy, test_size=test_size, random_state=42) \n",
    "                \n",
    "                #OLS regression, using matrix inversion\n",
    "                OLSbeta = np.linalg.inv(X_train.T @ X_train) @ X_train.T @ y_train  \n",
    "                #print(OLSbeta)\n",
    "                ytilde = X_train @ OLSbeta \n",
    "                ypredict = X_test @ OLSbeta #<----I'm doing it here analytically, just to show what's inside the LinearRegression function. Doing a LinearRegression.predict at the test set yields the same results.\n",
    "                #MSE prediction\n",
    "                MSE_train_iter[iter]=MSE(y_train,ytilde) \n",
    "                MSE_test_iter[iter]=MSE(y_test,ypredict) \n",
    "                \n",
    "                #Correlation prediction\n",
    "                R2_train_iter[iter]=R2(y_train,ytilde) #train data, original vs predicted\n",
    "                R2_test_iter[iter]=R2(y_test,ypredict) #test data original vs predicted\n",
    "\n",
    "            #MSE prediction\n",
    "            MSE_train[degree]=np.mean(MSE_train_iter) #train data, original vs predicted\n",
    "            #print(MSE_train)\n",
    "            MSE_test[degree]=np.mean(MSE_test_iter) #test data original vs predicted\n",
    "            \n",
    "            #Correlation prediction\n",
    "            R2_train[degree]=np.mean(R2_train_iter) #train data, original vs predicted\n",
    "            #print(R2_train)\n",
    "            R2_test[degree]=np.mean(R2_test_iter) #test data original vs predicted\n",
    "            \n",
    "            if n==200 and test_size==0.25 and degree==6:\n",
    "                print(OLSbeta)\n",
    "\n",
    "        plt.plot(polydegree, MSE_train, label='MSE train')\n",
    "        plt.plot(polydegree, MSE_test, label='MSE test')\n",
    "        plt.xlabel('Pol. Degree')\n",
    "        plt.ylabel('MSE')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "            \n",
    "        plt.plot(polydegree, R2_test, label='R2 Test')\n",
    "        plt.plot(polydegree, R2_train, label='R2 Train')\n",
    "        plt.xlabel('Pol. Degree')\n",
    "        plt.ylabel('R2')\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82448cd6-c422-4ffb-a4eb-fade557806b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bootstrap OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6a698c-670f-473f-8cde-ee279930033a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bias Variance trade-off as a function of the model's complexity\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import resample\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def MSE(y_data,y_model):\n",
    "    n = np.size(y_model)\n",
    "    return np.sum((y_data-y_model)**2)/n\n",
    "\n",
    "MSE_Bootstraps_compare = np.zeros(maxdegree) #to compare with the C-V code later\n",
    "\n",
    "#varying the grid size:\n",
    "for n in [200, 500, 1000]:    \n",
    "\n",
    "    #maximum polynomial degree+1\n",
    "    maxdegree = 9\n",
    "    \n",
    "    for n_bootstraps in [1, 20, 100]:\n",
    "        print('number of datapoints:', n,', number of bootstraps:', n_bootstraps)\n",
    "\n",
    "        # Make data set.\n",
    "        x = np.linspace(0,(np.pi)/2,n).reshape(-1, 1)\n",
    "\n",
    "        #Northern H. parameters\n",
    "        s0 = 1\n",
    "        s2 = -0.473\n",
    "        a0 = 0.675\n",
    "        a2 = -0.192\n",
    "        i2 = -0.165\n",
    "\n",
    "        #flux function (eqn. (14) from Stone_1978)\n",
    "        y = 0.5*(s0*a2+s2*a0+(2/7)*s2*a2-i2)*((np.sin(x))**3-np.sin(x))\n",
    "\n",
    "        #noisy flux function\n",
    "        y_noisy = np.random.normal(y, abs(y*0.05))\n",
    "\n",
    "        MSE_Bootstrap = np.zeros(maxdegree)\n",
    "\n",
    "        bias = np.zeros(maxdegree)\n",
    "        variance = np.zeros(maxdegree)\n",
    "        polydegree = np.zeros(maxdegree)\n",
    "\n",
    "        #splitting of the data\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x, y_noisy, test_size=0.25, random_state=42) \n",
    "                                                                                        # the fact that I'm resplitting the data for each degree loop shouldn't and doesn't affect the final value of MSE \n",
    "                                                                                        #I want to keep the splitting out of the loop because of the resampling. It doens't seem to affect the results, though...\n",
    "        for degree in range(maxdegree):\n",
    "\n",
    "            #polynomial fit\n",
    "            polydegree[degree] = degree\n",
    "            model = make_pipeline(PolynomialFeatures(degree=degree), LinearRegression(fit_intercept=False)) #it's easier to use Python's Pol.fit and Lin. Regr. features for this part now. \n",
    "            ypredict = np.empty((y_test.shape[0], n_bootstraps))    \n",
    "\n",
    "            for i in range(n_bootstraps):\n",
    "                x_, y_ = resample(x_train, y_train)\n",
    "\n",
    "                #applying Linear Regression to the training data set\n",
    "                ypredict[:, i] = model.fit(x_, y_).predict(x_test).ravel()     \n",
    "\n",
    "            MSE_Bootstrap[degree] = MSE(y_test, ypredict)\n",
    "            \n",
    "            if (n==200 and n_bootstraps == 20):\n",
    "                MSE_Bootstraps_compare[degree]=MSE_Bootstrap[degree]#to compare with the C-V code later\n",
    "                #print(MSE_Bootstraps_compare)\n",
    "                \n",
    "            bias[degree] = np.mean( (y_test - np.mean(ypredict, axis=1, keepdims=True))**2 )\n",
    "            variance[degree] = np.mean( np.var(ypredict, axis=1, keepdims=True) )\n",
    "\n",
    "        plt.plot(polydegree, MSE_Bootstrap, label='MSE')\n",
    "        plt.plot(polydegree, bias, label='bias')\n",
    "        plt.plot(polydegree, variance, label='Variance')\n",
    "        plt.xlabel('Pol. Degree')\n",
    "        plt.ylabel('Prediction Error')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "#print(MSE_Bootstraps_compare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d293fa7-9ed6-422d-b180-b3c917ee5eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross validation OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfb260c-f545-49aa-90b6-ca1aa1f4444d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def MSE(y_data,y_model):\n",
    "    n = np.size(y_model)\n",
    "    return np.sum((y_data-y_model)**2)/n\n",
    "\n",
    "#grid size\n",
    "n = 200 \n",
    "#maximum polynomial degree+1\n",
    "maxdegree = 9\n",
    "\n",
    "# Make data set.\n",
    "x = np.linspace(0,(np.pi)/2,n)#.reshape(-1, 1)\n",
    "\n",
    "#Northern H. parameters\n",
    "s0 = 1\n",
    "s2 = -0.473\n",
    "a0 = 0.675\n",
    "a2 = -0.192\n",
    "i2 = -0.165\n",
    "\n",
    "#flux function (eqn. (14) from Stone_1978)\n",
    "y = 0.5*(s0*a2+s2*a0+(2/7)*s2*a2-i2)*((np.sin(x))**3-np.sin(x))\n",
    "\n",
    "#noisy flux function\n",
    "y_noisy = np.random.normal(y, abs(y*0.05))\n",
    "\n",
    "polydegree = np.zeros(maxdegree)\n",
    "model = LinearRegression(fit_intercept=False)\n",
    "\n",
    "# Number of folds\n",
    "for k in [5, 10, n]: #n for the LOO-CV\n",
    "    kfold = KFold(n_splits = k)\n",
    "\n",
    "    # Perform the cross-validation to estimate MSE\n",
    "    MSE_KFold = np.zeros((maxdegree, k))\n",
    "\n",
    "    i = 0\n",
    "    for degree in range(maxdegree):\n",
    "        poly = PolynomialFeatures(degree=degree)\n",
    "        polydegree[degree]=degree\n",
    "        j = 0\n",
    "        for train_inds, test_inds in kfold.split(x):\n",
    "            xtrain = x[train_inds]\n",
    "            ytrain = y_noisy[train_inds]\n",
    "\n",
    "            xtest = x[test_inds]\n",
    "            ytest = y_noisy[test_inds]\n",
    "\n",
    "            Xtrain = poly.fit_transform(xtrain[:, np.newaxis])\n",
    "            model.fit(Xtrain, ytrain[:, np.newaxis])\n",
    "\n",
    "            Xtest = poly.fit_transform(xtest[:, np.newaxis])\n",
    "            ypred = model.predict(Xtest)\n",
    "\n",
    "            MSE_KFold[i,j] = MSE(ytest[:, np.newaxis], ypred)\n",
    "\n",
    "            j += 1\n",
    "        i += 1\n",
    "\n",
    "    estimated_mse_KFold = np.mean(MSE_KFold, axis = 1)\n",
    "\n",
    "    # Cross-validation using cross_val_score from sklearn along with KFold\n",
    "\n",
    "    estimated_mse_sklearn = np.zeros(maxdegree)\n",
    "    i = 0\n",
    "    for degree in range(maxdegree):\n",
    "        polydegree[degree]=degree\n",
    "        poly = PolynomialFeatures(degree=degree)\n",
    "        X = poly.fit_transform(x[:, np.newaxis])\n",
    "        estimated_mse_folds = cross_val_score(model, X, y_noisy[:, np.newaxis], scoring='neg_mean_squared_error', cv=k)\n",
    "\n",
    "        # cross_val_score return an array containing the estimated NEGATIVE mse for every fold.\n",
    "        # we have to the the mean of every array in order to get an estimate of the mse of the model\n",
    "        estimated_mse_sklearn[i] = np.mean(-estimated_mse_folds)\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    ## Plot    \n",
    "        \n",
    "    if k==5:          \n",
    "        plt.figure()\n",
    "        plt.plot(polydegree, estimated_mse_sklearn, 'b', label = 'cross_val_score')\n",
    "        plt.plot(polydegree, estimated_mse_KFold, 'r--',  label = 'MSE KFold, k=5')\n",
    "        plt.plot(polydegree, MSE_Bootstraps_compare, label='MSE Bootstrap, n=200, #bootstraps= 20')\n",
    "        plt.xlabel('Pl. Degree')\n",
    "        plt.ylabel('MSE')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    elif k==10:\n",
    "        plt.figure()\n",
    "        plt.plot(polydegree, estimated_mse_sklearn, 'b', label = 'cross_val_score')\n",
    "        plt.plot(polydegree, estimated_mse_KFold, 'r--',  label = 'MSE KFold, k=10')\n",
    "        plt.plot(polydegree, MSE_Bootstraps_compare, label='MSE Bootstrap,n=200, #bootstraps= 20')\n",
    "        plt.xlabel('Pl. Degree')\n",
    "        plt.ylabel('MSE')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.figure()\n",
    "        plt.plot(polydegree, estimated_mse_sklearn, 'b', label = 'cross_val_score')\n",
    "        plt.plot(polydegree, estimated_mse_KFold, 'r--',  label = 'MSE LOO')\n",
    "        plt.plot(polydegree, MSE_Bootstraps_compare, label='MSE Bootstrap,n=200, #bootstraps= 20')\n",
    "        plt.xlabel('Pl. Degree')\n",
    "        plt.ylabel('MSE')\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
