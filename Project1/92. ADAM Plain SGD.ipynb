{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1be0f7-dc60-49f1-bcfa-c90c35a2f8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6ai) ADAM SGD OLS  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a68a687-88e2-4db2-9469-a737845c836d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing various packages\n",
    "from random import random, seed\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "import sys\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from math import exp, sqrt\n",
    "from random import random, seed\n",
    "\n",
    "def MSE(y_data,y_model):\n",
    "    n = np.size(y_model)\n",
    "    return np.sum((y_data-y_model)**2)/n\n",
    "\n",
    "#grid size\n",
    "n = 200 \n",
    "\n",
    "# Make data set.\n",
    "x = np.linspace(0,(np.pi)/2,n).reshape(-1, 1)\n",
    "\n",
    "#Northern H. parameters\n",
    "s0 = 1\n",
    "s2 = -0.473\n",
    "a0 = 0.675\n",
    "a2 = -0.192\n",
    "i2 = -0.165\n",
    "\n",
    "#flux function (eqn. (14) from Stone_1978)\n",
    "y = 0.5*(s0*a2+s2*a0+(2/7)*s2*a2-i2)*((np.sin(x))**3-np.sin(x))\n",
    "\n",
    "#noisy flux function\n",
    "y_noisy = np.random.normal(y, abs(y*0.05)) \n",
    "\n",
    "#polynomial fit\n",
    "degree=6\n",
    "poly = PolynomialFeatures(degree=degree)\n",
    "X = poly.fit_transform(x)\n",
    "\n",
    "# Hessian matrix\n",
    "H = (2.0/n)* X.T @ X\n",
    "invH = np.linalg.pinv(H)\n",
    "\n",
    "# Get the eigenvalues\n",
    "EigValues, EigVectors = np.linalg.eig(H)\n",
    "\n",
    "n_epochs = 100000\n",
    "\n",
    "nsizes= 4\n",
    "sizes=[1, 2, 4, 8]\n",
    "\n",
    "ngammas = 4 \n",
    "gammas = np.logspace(-3, -1, ngammas)\n",
    "\n",
    "beta_OLS_array = np.zeros((nsizes, ngammas), dtype=object)\n",
    "y_OLS_SGD_adam_array = np.zeros((nsizes, ngammas), dtype=object)\n",
    "MSE_OLS_SGD_adam = np.zeros((nsizes, ngammas))\n",
    "    \n",
    "for s in range(nsizes):\n",
    "    M=sizes[s] #we vary the size of the minibatches\n",
    "    m = int(n/M) #number of minibatches\n",
    "    \n",
    "    for g in range(ngammas):\n",
    "        gamma = gammas[g]\n",
    "\n",
    "        beta_OLS = np.random.randn(degree+1,1)\n",
    "\n",
    "        # Value for parameter rho_1\n",
    "        rho_1 = 0.9\n",
    "        # Value for parameter rho_2\n",
    "        rho_2 = 0.999\n",
    "        # Including AdaGrad parameter to avoid possible division by zero\n",
    "        delta  = 1e-8\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "            Giter_1 = np.zeros(shape=(degree+1,1))\n",
    "            Giter_2 = np.zeros(shape=(degree+1,1))\n",
    "\n",
    "            for i in range(m):\n",
    "                random_index = M*np.random.randint(m)\n",
    "                xi = X[random_index:random_index+M]\n",
    "                yi = y_noisy[random_index:random_index+M]\n",
    "\n",
    "                gradient_OLS = (2.0/M)* xi.T @ ((xi @ beta_OLS)-yi)\n",
    "                #print(np.shape(gradient_OLS))\n",
    "\n",
    "                Previous_1= Giter_1\n",
    "                #print(np.shape(Previous_1))\n",
    "\n",
    "                Giter_1= gradient_OLS\n",
    "\n",
    "                #update 1st momentum:        \n",
    "                Gnew_1 = rho_1*Previous_1+(1-rho_1)*gradient_OLS\n",
    "                #print(np.shape(Gnew_1))\n",
    "\n",
    "                # Previous value for the outer product of gradients\n",
    "                Previous_2 = Giter_2\n",
    "                \n",
    "                # Accumulated gradient\n",
    "                Giter_2 += np.multiply(gradient_OLS, gradient_OLS)# @ gradient_OLS.T\n",
    "\n",
    "                #update 2nd momentum:\n",
    "                Gnew_2 = rho_2*Previous_2+(1-rho_2)*Giter_2\n",
    "\n",
    "                #correction of 1st momentum bias:\n",
    "                Gnew_1_bias= Gnew_1/(1.0-rho_1**epoch+1)\n",
    "\n",
    "                #correction of 2nd momentum bias:\n",
    "                Gnew_2_bias= Gnew_2/(1.0-rho_2**epoch+1)        \n",
    "\n",
    "                # Hadamard product\n",
    "                update = np.c_[gamma*Gnew_1_bias/(delta+np.sqrt(np.diagonal(Gnew_2_bias)))]\n",
    "\n",
    "                beta_OLS -= update\n",
    "                \n",
    "        #storing every β for each M, γ combination\n",
    "        beta_OLS_array[s, g] =beta_OLS\n",
    "        \n",
    "        y_OLS_SGD_adam = X @ beta_OLS\n",
    "        \n",
    "        y_OLS_SGD_adam_array[s, g] = y_OLS_SGD_adam\n",
    "\n",
    "        MSE_OLS_SGD_adam[s, g] = MSE(y_OLS_SGD_adam, y_noisy)\n",
    "        \n",
    "#finding the minimum value of the MSE\n",
    "MSE_OLS_SGD_adam_optimal = np.min(MSE_OLS_SGD_adam)\n",
    "conditon = (MSE_OLS_SGD_adam == MSE_OLS_SGD_adam_optimal)\n",
    "#the l, g for which we have the minimum MSE\n",
    "result = np.where(conditon)\n",
    "print(result)\n",
    "\n",
    "print('method MSE=', MSE_OLS_SGD_adam_optimal)  \n",
    "\n",
    "#saving the y that gives the optimal MSE\n",
    "y_OLS_SGD_adam_optimal= y_OLS_SGD_adam_array[result]\n",
    "y_OLS_SGD_adam_optimal=(y_OLS_SGD_adam_optimal[0])\n",
    "\n",
    "#printing the final β \n",
    "beta_OLS_optimal = beta_OLS_array[result]\n",
    "print(beta_OLS_optimal)\n",
    "        \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "pd.DataFrame(MSE_OLS_SGD_adam)\n",
    "mse_data_ols_sgd_adam = pd.DataFrame(MSE_OLS_SGD_adam)\n",
    "\n",
    "plt.figure(figsize=(12,7))\n",
    "sns.heatmap(data=mse_data_ols_sgd_adam, annot=True,  fmt=\".1e\", cmap=\"crest\")\n",
    "plt.xlabel(\"initial γ\")\n",
    "plt.ylabel(\"number of minibatches\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x*180/np.pi, y_noisy, 'ro', label='data')\n",
    "plt.plot(x*180/np.pi, y_OLS_SGD_adam_optimal, label='ADAM SGD OLS')\n",
    "plt.xlabel('latitude [degrees]')\n",
    "plt.ylabel('f')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#Calculating the time processing time\n",
    "import time\n",
    "start_time = time.time()\n",
    "print(\"execution time=\", (time.time() - start_time), 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95d5556-2cda-421e-b979-5f739466e8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6aii) ADAM SGD Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119024f4-40a3-475a-beb8-bc6428b8b34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ridge parameter\n",
    "nlambdas = 6 \n",
    "lambdas = np.logspace(-5, 0, nlambdas)\n",
    "\n",
    "ngammas = 6 \n",
    "gammas = np.logspace(-3, 1, ngammas)\n",
    "\n",
    "n_epochs = 10000\n",
    "\n",
    "for M in [1, 2, 4]:  \n",
    "    m = int(n/M) #number of minibatches\n",
    "\n",
    "    beta_Ridge = np.random.randn(degree+1,1)\n",
    "    \n",
    "    y_Ridge_SGD_adam_array = np.zeros((nlambdas, ngammas), dtype=object)\n",
    "    MSE_Ridge_SGD_adam = np.zeros((nlambdas, ngammas))\n",
    "\n",
    "    # Value for parameter rho_1\n",
    "    rho_1 = 0.9\n",
    "    # Value for parameter rho_2\n",
    "    rho_2 = 0.999\n",
    "    \n",
    "    # Including AdaGrad parameter to avoid possible division by zero\n",
    "    delta  = 1e-8\n",
    "\n",
    "    for l in range(nlambdas):\n",
    "        lmbda = lambdas[l]\n",
    "        beta_Ridge = np.random.randn(degree+1,1)    \n",
    "                \n",
    "        # We vary the learning rate\n",
    "        for g in range(ngammas):\n",
    "            gamma=gammas[g] \n",
    "\n",
    "            for epoch in range(n_epochs):\n",
    "                Giter_1 = np.zeros(shape=(degree+1,1))    \n",
    "                Giter_2 = np.zeros(shape=(degree+1,1))\n",
    "\n",
    "                for i in range(m):\n",
    "                    random_index = M*np.random.randint(m)\n",
    "                    xi = X[random_index:random_index+M]\n",
    "                    yi = y_noisy[random_index:random_index+M]\n",
    "\n",
    "                    gradient_Ridge = (2.0/M)*xi.T @ (xi @ (beta_Ridge)-yi)+2*lmbda*beta_Ridge\n",
    "\n",
    "                    Previous_1= Giter_1\n",
    "                    #print(np.shape(Previous_1))\n",
    "\n",
    "                    Giter_1= gradient_Ridge\n",
    "\n",
    "                    #update 1st momentum:        \n",
    "                    Gnew_1 = rho_1*Previous_1+(1-rho_1)*gradient_Ridge\n",
    "                    #print(np.shape(Gnew_1))\n",
    "\n",
    "                    # Previous value for the outer product of gradients\n",
    "                    Previous_2 = Giter_2\n",
    "\n",
    "                    # Accumulated gradient\n",
    "                    Giter_2 += np.multiply(gradient_Ridge, gradient_Ridge)# @ gradient_OLS.T\n",
    "\n",
    "                    #update 2nd momentum:\n",
    "                    Gnew_2 = rho_2*Previous_2+(1-rho_2)*Giter_2\n",
    "\n",
    "                    #correction of 1st momentum bias:\n",
    "                    Gnew_1_bias= Gnew_1/(1.0-rho_1**epoch+1)#<-or maybe epochs??see the video\n",
    "                    #print(np.shape(Gnew_1_bias))\n",
    "\n",
    "                    #correction of 2nd momentum bias:\n",
    "                    Gnew_2_bias= Gnew_2/(1.0-rho_2**epoch+1)        \n",
    "\n",
    "                    # Hadamard product\n",
    "                    update = np.c_[gamma*Gnew_1_bias/(delta+np.sqrt(np.diagonal(Gnew_2_bias)))]\n",
    "\n",
    "                    beta_Ridge -= update\n",
    "\n",
    "            y_Ridge_SGD_adam = X @ beta_Ridge\n",
    "            \n",
    "            y_Ridge_SGD_adam_array[l, g]= y_Ridge_SGD_adam\n",
    "\n",
    "            MSE_Ridge_SGD_adam[l, g]=MSE(y_noisy, y_Ridge_SGD_adam)    \n",
    "\n",
    "    #finding the minimum value of the MSE\n",
    "    MSE_Ridge_SGD_adam_optimal = np.min(MSE_Ridge_SGD_adam)\n",
    "    conditon = (MSE_Ridge_SGD_adam == MSE_Ridge_SGD_adam_optimal)\n",
    "    #the l, g for which we have the minimum MSE\n",
    "    result = np.where(conditon)\n",
    "    print(result)\n",
    "\n",
    "    print('method MSE=', MSE_Ridge_SGD_adam_optimal)  \n",
    "\n",
    "    #saving the y that gives the optimal MSE\n",
    "    y_Ridge_SGD_adam_optimal= y_Ridge_SGD_adam_array[result]\n",
    "    y_Ridge_SGD_adam_optimal=(y_Ridge_SGD_adam_optimal[0])\n",
    "\n",
    "    pd.DataFrame(MSE_Ridge_SGD_adam)\n",
    "    mse_data_ridge_sgd_adam = pd.DataFrame(MSE_Ridge_SGD_adam)\n",
    "\n",
    "    plt.figure(figsize=(12,7))\n",
    "    sns.heatmap(data=mse_data_ridge_sgd_adam, annot=True,  fmt=\".1e\", cmap=\"crest\")\n",
    "    plt.xlabel(\"γ\")\n",
    "    plt.ylabel(\"λ\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(x*180/np.pi, y_noisy, 'ro', label='data')\n",
    "    plt.plot(x*180/np.pi, y_Ridge_SGD_adam_optimal, label='ADAM SGD Ridge')\n",
    "    plt.xlabel('latitude [degrees]')\n",
    "    plt.ylabel('f')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "#Calculating the time processing time\n",
    "import time\n",
    "start_time = time.time()\n",
    "print(\"execution time=\", (time.time() - start_time), 's')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
