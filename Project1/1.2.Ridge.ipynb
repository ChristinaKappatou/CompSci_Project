{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee2e851-b5c5-43c8-955a-a930162110bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plain Ridge regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5eb669-4c16-4bc3-b3c4-bcbeae1bda70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def MSE(y_data,y_model):\n",
    "    n = np.size(y_model)\n",
    "    return np.sum((y_data-y_model)**2)/n\n",
    "\n",
    "def R2(y_data, y_model):\n",
    "    return 1 - np.sum((y_data - y_model) ** 2) / np.sum((y_data - np.mean(y_data)) ** 2)\n",
    "\n",
    "#grid size\n",
    "n = 200 \n",
    "\n",
    "# Make data set.\n",
    "x = np.linspace(0,(np.pi)/2,n).reshape(-1, 1)\n",
    "\n",
    "#Northern H. parameters\n",
    "s0 = 1\n",
    "s2 = -0.473\n",
    "a0 = 0.675\n",
    "a2 = -0.192\n",
    "i2 = -0.165\n",
    "\n",
    "#flux function (eqn. (14) from Stone_1978)\n",
    "y = 0.5*(s0*a2+s2*a0+(2/7)*s2*a2-i2)*((np.sin(x))**3-np.sin(x))\n",
    "\n",
    "#noisy flux function\n",
    "y_noisy = np.random.normal(y, abs(y*0.05)) \n",
    "\n",
    "maxdegree=10\n",
    "MSE_difference = np.zeros((maxdegree, nlambdas))\n",
    "\n",
    "for degree in range(maxdegree):\n",
    "    #polynomial fit\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "    X = poly.fit_transform(x)\n",
    "\n",
    "    # Repeat now for Ridge regression and various values of the regularization parameter, Î»\n",
    "    I = np.eye(1,1)\n",
    "\n",
    "    # Decide which values of lambda to use\n",
    "    nlambdas = 15\n",
    "\n",
    "    MSEPredict = np.zeros(nlambdas)\n",
    "    MSEfit = np.zeros(nlambdas)\n",
    "    \n",
    "    R2Predict = np.zeros(nlambdas)\n",
    "    R2fit = np.zeros(nlambdas)   \n",
    "    \n",
    "    \n",
    "    lambdas = np.logspace(-4, 10, nlambdas)\n",
    "    \n",
    "    for i in range(nlambdas):\n",
    "        \n",
    "        lmb = lambdas[i]\n",
    "        \n",
    "        #splitting of the data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y_noisy, test_size=0.25, random_state=42)     \n",
    "\n",
    "        Ridgebeta = np.linalg.inv(X_train.T @ X_train+lmb*I) @ X_train.T @ y_train\n",
    "\n",
    "        # Prediction\n",
    "        ypredict = X_test @ Ridgebeta\n",
    "        yfit = X_train @ Ridgebeta\n",
    "       \n",
    "        MSEPredict[i]=MSE(y_test,ypredict)\n",
    "        #print('predict', MSEPredict)\n",
    "        MSEfit[i]=MSE(y_train,yfit) \n",
    "        #print('fit', MSEfit)\n",
    "        R2Predict[i]= R2(y_test,ypredict)\n",
    "        R2fit[i]=R2(y_train,yfit)\n",
    "\n",
    "        MSE_difference[degree, i]= np.abs(MSEPredict[i]-MSEfit[i])\n",
    "        #print(MSE_difference)\n",
    "        \n",
    "        if i==5 and degree==6:\n",
    "            print(Ridgebeta)\n",
    "\n",
    "    if degree in [1, 6, 9]:    \n",
    "        #I'm plotting the prediction\n",
    "        print(degree)\n",
    "        plt.figure()\n",
    "        #plt.ylim([0, 0.00125])\n",
    "        plt.plot(np.log10(lambdas), MSEPredict, label = 'MSE test Ridge')\n",
    "        plt.plot(np.log10(lambdas), MSEfit, label = 'MSE train Ridge')\n",
    "        plt.xlabel('log10(lambda)')\n",
    "        plt.ylabel('MSE')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "#    plt.figure()\n",
    "#    plt.plot(np.log10(lambdas), R2Predict, label = 'R2 test Ridge')\n",
    "#    plt.plot(np.log10(lambdas), R2fit, label = 'R2 train Ridge')\n",
    "#    plt.xlabel('log10(lambda)')\n",
    "#    plt.ylabel('MSE')\n",
    "#    plt.legend()\n",
    "#    plt.show()\n",
    "\n",
    "#print(MSE_difference)\n",
    "\n",
    "pd.DataFrame(MSE_difference)\n",
    "mse_difference = pd.DataFrame(MSE_difference)\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "sns.heatmap(data=mse_difference, annot=True,  fmt=\"1.1e\", cmap=\"crest\")\n",
    "plt.xlabel(\"lambda number\")\n",
    "plt.ylabel(\"pol.degree\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0852f5-62b0-4a9c-a6ec-105ae47435a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bootstrap resampling on Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f785a70f-f415-4472-8736-00dc2e2567c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import resample\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def MSE(y_data,y_model):\n",
    "    n = np.size(y_model)\n",
    "    return np.sum((y_data-y_model)**2)/n\n",
    "\n",
    "#grid size\n",
    "n = 200 \n",
    "\n",
    "# Make data set.\n",
    "x = np.linspace(0,(np.pi)/2,n).reshape(-1, 1)\n",
    "\n",
    "#Northern H. parameters\n",
    "s0 = 1\n",
    "s2 = -0.473\n",
    "a0 = 0.675\n",
    "a2 = -0.192\n",
    "i2 = -0.165\n",
    "\n",
    "#flux function (eqn. (14) from Stone_1978)\n",
    "y = 0.5*(s0*a2+s2*a0+(2/7)*s2*a2-i2)*((np.sin(x))**3-np.sin(x))\n",
    "\n",
    "#noisy flux function\n",
    "y_noisy = np.random.normal(y, abs(y*0.05)) \n",
    "\n",
    "MSE_Bootstrap_compare_d1=np.zeros(nlambdas)\n",
    "MSE_Bootstrap_compare_d6=np.zeros(nlambdas)\n",
    "MSE_Bootstrap_compare_d9=np.zeros(nlambdas)\n",
    "\n",
    "for degree in [1, 6, 9]:\n",
    "\n",
    "    #Polynomial fit but now for degree=ct.=6\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "    X = poly.fit_transform(x)\n",
    "\n",
    "    #splitting of the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_noisy, test_size=0.25, random_state=42) \n",
    "\n",
    "    # Decide which values of lambda to use\n",
    "    nlambdas = 100\n",
    "    lambdas = np.logspace(-4, 10, nlambdas)\n",
    "\n",
    "    #number of bootstrap samples\n",
    "    #n_boostraps = 15 \n",
    "    \n",
    "    \n",
    "    for n_bootstraps in [1, 20, 100]:\n",
    "        print('polynomial degree:', degree,', number of bootstraps:', n_bootstraps)\n",
    "        \n",
    "        MSE_Bootstrap = np.zeros(nlambdas)\n",
    "        bias = np.zeros(nlambdas)\n",
    "        variance = np.zeros(nlambdas)\n",
    "\n",
    "        for i in range(nlambdas):\n",
    "            lmb = lambdas[i]\n",
    "            model = Ridge(alpha = lmb)\n",
    "            ypredict = np.empty((y_test.shape[0], n_bootstraps))    \n",
    "\n",
    "            for j in range(n_bootstraps):\n",
    "                x_, y_ = resample(X_train, y_train)\n",
    "\n",
    "                #applying Linear Regression to the training data set\n",
    "                ypredict[:, j] = model.fit(x_, y_).predict(X_test).ravel()       \n",
    "\n",
    "            MSE_Bootstrap[i] = MSE(y_test, ypredict)\n",
    "            \n",
    "            if n_bootstraps==20 and degree==1:\n",
    "                MSE_Bootstrap_compare_d1=MSE_Bootstrap\n",
    "            elif n_bootstraps==20 and degree==6:\n",
    "                MSE_Bootstrap_compare_d6=MSE_Bootstrap\n",
    "            elif n_bootstraps==20 and degree==9:\n",
    "                MSE_Bootstrap_compare_d9=MSE_Bootstrap\n",
    "                \n",
    "                \n",
    "            bias[i] = np.mean( (y_test - np.mean(ypredict, axis=1, keepdims=True))**2 )\n",
    "            variance[i] = np.mean( np.var(ypredict, axis=1, keepdims=True) )\n",
    "\n",
    "        plt.plot(np.log10(lambdas), MSE_Bootstrap, label='MSE')\n",
    "        plt.plot(np.log10(lambdas), bias, label='bias')\n",
    "        plt.plot(np.log10(lambdas), variance, label='Variance')\n",
    "        plt.xlabel('log10(lambda)')\n",
    "        plt.ylabel('Prediction Error')\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdcd471-2a78-45d0-8a2a-b8a8fb336863",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross-validation on Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85c0ea7-8a99-4314-b269-71670b2d897a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import resample\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def MSE(y_data,y_model):\n",
    "    n = np.size(y_model)\n",
    "    return np.sum((y_data-y_model)**2)/n\n",
    "\n",
    "#grid size\n",
    "n = 200 \n",
    "\n",
    "# Make data set.\n",
    "x = np.linspace(0,(np.pi)/2,n)#.reshape(-1, 1)\n",
    "\n",
    "#Northern H. parameters\n",
    "s0 = 1\n",
    "s2 = -0.473\n",
    "a0 = 0.675\n",
    "a2 = -0.192\n",
    "i2 = -0.165\n",
    "\n",
    "#flux function (eqn. (14) from Stone_1978)\n",
    "y = 0.5*(s0*a2+s2*a0+(2/7)*s2*a2-i2)*((np.sin(x))**3-np.sin(x))\n",
    "\n",
    "#noisy flux function\n",
    "y_noisy = np.random.normal(y, abs(y*0.05))\n",
    "\n",
    "for degree in [1, 6, 9]:\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "\n",
    "    # Decide which values of lambda to use\n",
    "    nlambdas = 100\n",
    "    lambdas = np.logspace(-4, 10, nlambdas)\n",
    "\n",
    "    for k in [5, 10, n]: #n for LOO C-V\n",
    "        print('pol. degree=', degree, 'k=', k)\n",
    "        kfold = KFold(n_splits = k)\n",
    "\n",
    "        # Perform the cross-validation to estimate MSE\n",
    "        MSE_KFold = np.zeros((nlambdas, k))\n",
    "\n",
    "        c = 0\n",
    "        for i in range(nlambdas):\n",
    "            lmb = lambdas[i]\n",
    "            model = Ridge(alpha = lmb)\n",
    "            j = 0\n",
    "            for train_inds, test_inds in kfold.split(x):\n",
    "                xtrain = x[train_inds]\n",
    "                ytrain = y_noisy[train_inds]\n",
    "\n",
    "                xtest = x[test_inds]\n",
    "                ytest = y_noisy[test_inds]\n",
    "\n",
    "                Xtrain = poly.fit_transform(xtrain[:, np.newaxis])\n",
    "                model.fit(Xtrain, ytrain[:, np.newaxis])\n",
    "\n",
    "                Xtest = poly.fit_transform(xtest[:, np.newaxis])\n",
    "                ypred = model.predict(Xtest)\n",
    "\n",
    "                MSE_KFold[c,j] = MSE(ytest[:, np.newaxis], ypred)\n",
    "\n",
    "                j += 1\n",
    "            c += 1\n",
    "\n",
    "        estimated_mse_KFold = np.mean(MSE_KFold, axis = 1)\n",
    "\n",
    "## Cross-validation using cross_val_score from sklearn along with KFold\n",
    "\n",
    "        estimated_mse_sklearn = np.zeros(nlambdas)\n",
    "\n",
    "        d = 0\n",
    "        for i in range(nlambdas):\n",
    "            lmb = lambdas[i]\n",
    "            model = Ridge(alpha = lmb)\n",
    "            X = poly.fit_transform(x[:, np.newaxis])\n",
    "            estimated_mse_folds = cross_val_score(model, X, y_noisy[:, np.newaxis], scoring='neg_mean_squared_error', cv=k)\n",
    "\n",
    "            # cross_val_score return an array containing the estimated negative mse for every fold.\n",
    "            # we have to the the mean of every array in order to get an estimate of the mse of the model\n",
    "            estimated_mse_sklearn[d] = np.mean(-estimated_mse_folds)\n",
    "\n",
    "            d += 1\n",
    "\n",
    "        ## Plot    \n",
    "\n",
    "        if k==5:          \n",
    "            plt.figure()\n",
    "            plt.plot(np.log10(lambdas), estimated_mse_sklearn, 'b', label = 'cross_val_score')\n",
    "            plt.plot(np.log10(lambdas), estimated_mse_KFold, 'r--',  label = 'MSE KFold, k=5')\n",
    "            if degree==1:\n",
    "                plt.plot(np.log10(lambdas), MSE_Bootstrap_compare_d1, label='MSE Bootstrap')\n",
    "            elif degree==6:\n",
    "                plt.plot(np.log10(lambdas), MSE_Bootstrap_compare_d6, label='MSE Bootstrap')\n",
    "            elif degree==9:\n",
    "                plt.plot(np.log10(lambdas), MSE_Bootstrap_compare_d9, label='MSE Bootstrap')\n",
    "                 \n",
    "            plt.xlabel('log10(lambda)')\n",
    "            plt.ylabel('MSE')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "        elif k==10:\n",
    "            plt.figure()\n",
    "            plt.plot(np.log10(lambdas), estimated_mse_sklearn, 'b', label = 'cross_val_score')\n",
    "            plt.plot(np.log10(lambdas), estimated_mse_KFold, 'r--',  label = 'MSE KFold, k=10')\n",
    "            if degree==1:\n",
    "                plt.plot(np.log10(lambdas), MSE_Bootstrap_compare_d1, label='MSE Bootstrap')\n",
    "            elif degree==6:\n",
    "                plt.plot(np.log10(lambdas), MSE_Bootstrap_compare_d6, label='MSE Bootstrap')\n",
    "            elif degree==9:\n",
    "                plt.plot(np.log10(lambdas), MSE_Bootstrap_compare_d9, label='MSE Bootstrap')\n",
    "            plt.xlabel('log10(lambda)')\n",
    "            plt.ylabel('MSE')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "        else:\n",
    "            plt.figure()\n",
    "            plt.plot(np.log10(lambdas), estimated_mse_sklearn, 'b', label = 'cross_val_score')\n",
    "            plt.plot(np.log10(lambdas), estimated_mse_KFold, 'r--',  label = 'MSE LOO')\n",
    "            if degree==1:\n",
    "                plt.plot(np.log10(lambdas), MSE_Bootstrap_compare_d1, label='MSE Bootstrap')\n",
    "            elif degree==6:\n",
    "                plt.plot(np.log10(lambdas), MSE_Bootstrap_compare_d6, label='MSE Bootstrap')\n",
    "            elif degree==9:\n",
    "                plt.plot(np.log10(lambdas), MSE_Bootstrap_compare_d9, label='MSE Bootstrap')\n",
    "            plt.xlabel('log10(lambda)')\n",
    "            plt.ylabel('MSE')\n",
    "            plt.legend()\n",
    "            plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
