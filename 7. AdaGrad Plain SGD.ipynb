{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddf4a01-eb70-4e86-ac3e-12b17bd4caa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4ci) Adagrad SGD OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686db3bb-b934-44e0-9dd5-a8ca0f7d1091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing various packages\n",
    "from random import random, seed\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "import sys\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from math import exp, sqrt\n",
    "from random import random, seed\n",
    "\n",
    "def MSE(y_data,y_model):\n",
    "    n= np.size(y_model)\n",
    "    return np.sum((y_data-y_model)**2)/n\n",
    "\n",
    "#grid size\n",
    "n = 200 \n",
    "\n",
    "# Make data set\n",
    "x = np.linspace(0,(np.pi)/2,n).reshape(-1, 1)\n",
    "\n",
    "#Northern H. parameters\n",
    "s0 = 1\n",
    "s2 = -0.473\n",
    "a0 = 0.675\n",
    "a2 = -0.192\n",
    "i2 = -0.165\n",
    "\n",
    "#flux function (eqn. (14) from Stone_1978)\n",
    "y = 0.5*(s0*a2+s2*a0+(2/7)*s2*a2-i2)*((np.sin(x))**3-np.sin(x))\n",
    "\n",
    "#noisy flux function\n",
    "y_noisy = np.random.normal(y, abs(y*0.05)) \n",
    "\n",
    "#polynomial fit\n",
    "degree=6\n",
    "poly = PolynomialFeatures(degree=degree)\n",
    "X = poly.fit_transform(x)\n",
    "\n",
    "# Hessian matrix\n",
    "H = (2.0/n)* X.T @ X\n",
    "invH = np.linalg.pinv(H)\n",
    "\n",
    "# Get the eigenvalues\n",
    "EigValues, EigVectors = np.linalg.eig(H)\n",
    "\n",
    "n_epochs = 10000 \n",
    "\n",
    "nsizes= 6\n",
    "sizes=[1, 2, 4, 8, 16, 32]\n",
    "\n",
    "ngammas = 5 \n",
    "gammas = np.logspace(-3, -2, ngammas)\n",
    "\n",
    "y_OLS_SGD_ada_array = np.zeros((nsizes, ngammas), dtype=object)\n",
    "MSE_OLS_SGD_ada = np.zeros((nsizes, ngammas))\n",
    "    \n",
    "for s in range(nsizes):\n",
    "    M=sizes[s] #we vary the size of the minibatches\n",
    "    \n",
    "    for g in range(ngammas):# we vary the value for the initial learning rate\n",
    "        gamma = gammas[g]\n",
    "\n",
    "        m = int(n/M) #number of minibatches\n",
    "\n",
    "        # Including AdaGrad parameter to avoid possible division by zero\n",
    "        delta  = 1e-8\n",
    "\n",
    "        beta_OLS = np.random.randn(degree+1,1)\n",
    "\n",
    "        for epoch in range(n_epochs): \n",
    "\n",
    "            Giter = np.zeros(shape=(degree+1,degree+1))\n",
    "\n",
    "            for i in range(m):\n",
    "                random_index = M*np.random.randint(m)\n",
    "                xi = X[random_index:random_index+M]\n",
    "                yi = y_noisy[random_index:random_index+M]\n",
    "\n",
    "                gradient_OLS = (2.0/M)* xi.T @ ((xi @ beta_OLS)-yi)\n",
    "\n",
    "                # Calculate the outer product of the gradients\n",
    "                Giter +=gradient_OLS @ gradient_OLS.T\n",
    "\n",
    "                # Simpler algorithm with only diagonal elements\n",
    "                Ginverse = np.c_[gamma/(delta+np.sqrt(np.diagonal(Giter)))]\n",
    "\n",
    "                # compute update\n",
    "                update = np.multiply(Ginverse,gradient_OLS)\n",
    "\n",
    "                beta_OLS -= update\n",
    "\n",
    "        y_OLS_SGD_ada = X @ beta_OLS\n",
    "        \n",
    "        y_OLS_SGD_ada_array[s, g] = y_OLS_SGD_ada\n",
    "        \n",
    "        MSE_OLS_SGD_ada[s, g] = MSE(y_OLS_SGD_ada, y_noisy)\n",
    "        \n",
    "#finding the minimum value of the MSE\n",
    "MSE_OLS_SGD_ada_optimal = np.min(MSE_OLS_SGD_ada)\n",
    "conditon = (MSE_OLS_SGD_ada == MSE_OLS_SGD_ada_optimal)\n",
    "#the l, g for which we have the minimum MSE\n",
    "result = np.where(conditon)\n",
    "print(result)\n",
    "\n",
    "print('method MSE=', MSE_OLS_SGD_ada_optimal)  \n",
    "\n",
    "#saving the y that gives the optimal MSE\n",
    "y_OLS_SGD_ada_optimal= y_OLS_SGD_ada_array[result]\n",
    "y_OLS_SGD_ada_optimal=(y_OLS_SGD_ada_optimal[0])\n",
    "#print(y_Ridge_GD_optimal)             \n",
    "        \n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "pd.DataFrame(MSE_OLS_SGD_ada)\n",
    "mse_data_ols_sgd_ada = pd.DataFrame(MSE_OLS_SGD_ada)\n",
    "\n",
    "plt.figure(figsize=(12,7))\n",
    "sns.heatmap(data=mse_data_ols_sgd_ada, annot=True,  fmt=\".1e\", cmap=\"crest\")\n",
    "plt.xlabel(\"initial γ\")\n",
    "plt.ylabel(\"number of minibatches\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x*180/np.pi, y_noisy, 'ro', label='data')\n",
    "plt.plot(x*180/np.pi, y_OLS_SGD_ada_optimal, label='Adagrad SGD OLS')\n",
    "plt.xlabel('latitude [degrees]')\n",
    "plt.ylabel('f')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#Calculating the time processing time\n",
    "import time\n",
    "start_time = time.time()\n",
    "print(\"execution time=\", (time.time() - start_time), 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e780cc14-2e8c-428c-837e-4bb010716c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4cii) Adagrad SGD Ridge  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b422b0e5-d936-4699-adfd-6ebe2ac5bd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ridge parameter\n",
    "nlambdas = 5 \n",
    "lambdas = np.logspace(-5, -1, nlambdas)\n",
    "\n",
    "ngammas = 6 \n",
    "gammas = np.logspace(-3, -1, ngammas)\n",
    "\n",
    "y_Ridge_SGD_ada_array = np.zeros((nlambdas, ngammas), dtype=object)\n",
    "MSE_Ridge_SGD_ada = np.zeros((nlambdas, ngammas))\n",
    "\n",
    "n_epochs = 1000\n",
    "M=1   \n",
    "m = int(n/M) #number of minibatches\n",
    "\n",
    "for l in range(nlambdas):\n",
    "    lmbda = lambdas[l]\n",
    "    beta_Ridge = np.random.randn(degree+1,1)    \n",
    "    MSE_Ridge_SGD = np.zeros(nlambdas)\n",
    "\n",
    "    # We vary the learning rate\n",
    "    for g in range(ngammas):\n",
    "        gamma=gammas[g]\n",
    "\n",
    "        # Including AdaGrad parameter to avoid possible division by zero\n",
    "        delta  = 1e-8    \n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "\n",
    "            Giter = np.zeros(shape=(degree+1,degree+1))    \n",
    "\n",
    "            for i in range(m):\n",
    "                random_index = M*np.random.randint(m)\n",
    "                xi = X[random_index:random_index+M]\n",
    "                yi = y_noisy[random_index:random_index+M]\n",
    "\n",
    "                gradient_Ridge = (2.0/M)*xi.T @ (xi @ (beta_Ridge)-yi)+2*lmbda*beta_Ridge\n",
    "\n",
    "                # Calculate the outer product of the gradients\n",
    "                Giter +=gradient_Ridge @ gradient_Ridge.T\n",
    "\n",
    "                # Simpler algorithm with only diagonal elements\n",
    "                Ginverse = np.c_[gamma/(delta+np.sqrt(np.diagonal(Giter)))]\n",
    "\n",
    "                # compute update\n",
    "                update = np.multiply(Ginverse,gradient_Ridge)\n",
    "                beta_Ridge -= update\n",
    "\n",
    "        y_Ridge_SGD_ada = X @ beta_Ridge\n",
    "        \n",
    "        y_Ridge_SGD_ada_array[l, g] = y_Ridge_SGD_ada\n",
    "\n",
    "        MSE_Ridge_SGD_ada[l, g]=MSE(y_noisy, y_Ridge_SGD_ada)\n",
    "               \n",
    "#finding the minimum value of the MSE\n",
    "MSE_Ridge_SGD_ada_optimal = np.min(MSE_Ridge_SGD_ada)\n",
    "conditon = (MSE_Ridge_SGD_ada == MSE_Ridge_SGD_ada_optimal)\n",
    "#the l, g for which we have the minimum MSE\n",
    "result = np.where(conditon)\n",
    "print(result)\n",
    "\n",
    "print('method MSE=', MSE_Ridge_SGD_ada_optimal)  \n",
    "\n",
    "#saving the y that gives the optimal MSE\n",
    "y_Ridge_SGD_ada_optimal= y_Ridge_SGD_ada_array[result]\n",
    "y_Ridge_SGD_ada_optimal=(y_Ridge_SGD_ada_optimal[0])\n",
    "            \n",
    "pd.DataFrame(MSE_Ridge_SGD_ada)\n",
    "mse_data_ridge_sgd_ada = pd.DataFrame(MSE_Ridge_SGD_ada)\n",
    "\n",
    "plt.figure(figsize=(12,7))\n",
    "sns.heatmap(data=mse_data_ridge_sgd_ada, annot=True,  fmt=\".1e\", cmap=\"crest\")\n",
    "plt.xlabel(\"γ\")\n",
    "plt.ylabel(\"λ\")\n",
    "plt.show()            \n",
    "\n",
    "plt.plot(x*180/np.pi, y_noisy, 'ro')\n",
    "plt.plot(x*180/np.pi, y_Ridge_SGD_ada_optimal, label = 'optimal Ridge SGD')\n",
    "plt.xlabel('latitude [degrees]')\n",
    "plt.ylabel('f')\n",
    "plt.legend()\n",
    "plt.show()        \n",
    "\n",
    "#Calculating the time processing time\n",
    "import time\n",
    "start_time = time.time()\n",
    "print(\"execution time=\", (time.time() - start_time), 's')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
