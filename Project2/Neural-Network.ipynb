{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd3b4df9",
   "metadata": {},
   "source": [
    "# Neural Networks in Python\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fd4fb5",
   "metadata": {},
   "source": [
    "In this notebook we will present a flexible object oriented codebase for a feed forward neural network, along with a demonstration of how to use it. Before we get into the details of the neural network, we will first present some implementations of various schedulers, cost functions and activation functions that can be used together with the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0eaf00e",
   "metadata": {},
   "source": [
    "## I. Schedulers\n",
    "The code below shows object oriented implementations of the Constant, Momentum, Adagrad, AdagradMomentum, RMS prop and Adam schedulers. All of the classes belong to the shared abstract Scheduler class, and share the update_change() and reset() methods allowing for any of the schedulers to be seamlessly used during the training stage, as will later be shown in the fit() method of the neural network. Update_change() only has one parameter, the gradient ($δ^l_ja^{l−1}_k$), and returns the change which will be subtracted from the weights. The reset() function takes no parameters, and resets the desired variables. For Constant and Momentum, reset does nothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbb1c207",
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "\n",
    "class Scheduler:\n",
    "    \"\"\"\n",
    "    Abstract class for Schedulers\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, eta):\n",
    "        self.eta = eta\n",
    "\n",
    "    # should be overwritten\n",
    "    def update_change(self, gradient):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # overwritten if needed\n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class Constant(Scheduler):\n",
    "    def __init__(self, eta):\n",
    "        super().__init__(eta)\n",
    "\n",
    "    def update_change(self, gradient):\n",
    "        return self.eta * gradient\n",
    "    \n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class Momentum(Scheduler):\n",
    "    def __init__(self, eta: float, momentum: float):\n",
    "        super().__init__(eta)\n",
    "        self.momentum = momentum\n",
    "        self.change = 0\n",
    "\n",
    "    def update_change(self, gradient):\n",
    "        self.change = self.momentum * self.change + self.eta * gradient\n",
    "        return self.change\n",
    "\n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "class Adagrad(Scheduler):\n",
    "    def __init__(self, eta):\n",
    "        super().__init__(eta)\n",
    "        self.G_t = None\n",
    "\n",
    "    def update_change(self, gradient):\n",
    "        delta = 1e-8  # avoid division ny zero\n",
    "\n",
    "        if self.G_t is None:\n",
    "            self.G_t = np.zeros((gradient.shape[0], gradient.shape[0]))\n",
    "\n",
    "        self.G_t += gradient @ gradient.T\n",
    "\n",
    "        G_t_inverse = 1 / (\n",
    "            delta + np.sqrt(np.reshape(np.diagonal(self.G_t), (self.G_t.shape[0], 1)))\n",
    "        )\n",
    "        return self.eta * gradient * G_t_inverse\n",
    "\n",
    "    def reset(self):\n",
    "        self.G_t = None\n",
    "\n",
    "\n",
    "class AdagradMomentum(Scheduler):\n",
    "    def __init__(self, eta, momentum):\n",
    "        super().__init__(eta)\n",
    "        self.G_t = None\n",
    "        self.momentum = momentum\n",
    "        self.change = 0\n",
    "\n",
    "    def update_change(self, gradient):\n",
    "        delta = 1e-8  # avoid division ny zero\n",
    "\n",
    "        if self.G_t is None:\n",
    "            self.G_t = np.zeros((gradient.shape[0], gradient.shape[0]))\n",
    "\n",
    "        self.G_t += gradient @ gradient.T\n",
    "\n",
    "        G_t_inverse = 1 / (\n",
    "            delta + np.sqrt(np.reshape(np.diagonal(self.G_t), (self.G_t.shape[0], 1)))\n",
    "        )\n",
    "        self.change = self.change * self.momentum + self.eta * gradient * G_t_inverse\n",
    "        return self.change\n",
    "\n",
    "    def reset(self):\n",
    "        self.G_t = None\n",
    "\n",
    "\n",
    "class RMS_prop(Scheduler):\n",
    "    def __init__(self, eta, rho):\n",
    "        super().__init__(eta)\n",
    "        self.rho = rho\n",
    "        self.second = 0.0\n",
    "\n",
    "    def update_change(self, gradient):\n",
    "        delta = 1e-8  # avoid division ny zero\n",
    "        self.second = self.rho * self.second + (1 - self.rho) * gradient * gradient\n",
    "        return self.eta * gradient / (np.sqrt(self.second + delta))\n",
    "\n",
    "    def reset(self):\n",
    "        self.second = 0.0\n",
    "\n",
    "\n",
    "class Adam(Scheduler):\n",
    "    def __init__(self, eta, rho, rho2):\n",
    "        super().__init__(eta)\n",
    "        self.rho = rho\n",
    "        self.rho2 = rho2\n",
    "        self.moment = 0\n",
    "        self.second = 0\n",
    "        self.n_epochs = 1\n",
    "\n",
    "    def update_change(self, gradient):\n",
    "        delta = 1e-8  # avoid division ny zero\n",
    "\n",
    "        self.moment = self.rho * self.moment + (1 - self.rho) * gradient\n",
    "        self.second = self.rho2 * self.second + (1 - self.rho2) * gradient * gradient\n",
    "\n",
    "        moment_corrected = self.moment / (1 - self.rho**self.n_epochs)\n",
    "        second_corrected = self.second / (1 - self.rho2**self.n_epochs)\n",
    "\n",
    "        return self.eta * moment_corrected / (np.sqrt(second_corrected + delta))\n",
    "\n",
    "    def reset(self):\n",
    "        self.n_epochs += 1\n",
    "        self.moment = 0\n",
    "        self.second = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b043fddf",
   "metadata": {},
   "source": [
    "### Usage of schedulers\n",
    "To initalize a scheduler, simply create the object and pass in the necessary parameters such as the learning rate and the momentum as shown below. As the Scheduler class is an abstract class it should not called directly, and will raise an error upon usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0b0fb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "momentum_scheduler = Momentum(eta=1e-3, momentum=0.9)\n",
    "adam_scheduler = Adam(eta=1e-3, rho=0.9, rho2=0.999)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8cc996",
   "metadata": {},
   "source": [
    "Here is a small example for how a segment of code using schedulers could look. This code works for any scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5895e189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before scheduler:\n",
      "weights=array([[1., 1., 1.],\n",
      "       [1., 1., 1.],\n",
      "       [1., 1., 1.]])\n",
      "\n",
      "After scheduler:\n",
      "weights=array([[0.993993  , 0.99399301, 0.99401572],\n",
      "       [0.99399302, 0.993993  , 0.99399301],\n",
      "       [0.993993  , 0.99399301, 0.99399301]])\n"
     ]
    }
   ],
   "source": [
    "weights = np.ones((3,3))\n",
    "print(f\"Before scheduler:\\n{weights=}\")\n",
    "\n",
    "epochs = 10\n",
    "for e in range(epochs):\n",
    "    gradient = np.random.rand(3, 3)\n",
    "    change = adam_scheduler.update_change(gradient)\n",
    "    weights = weights - change\n",
    "    adam_scheduler.reset()\n",
    "\n",
    "print(f\"\\nAfter scheduler:\\n{weights=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7fa586",
   "metadata": {},
   "source": [
    "## II. Cost functions\n",
    "In this section we will quickly look at the cost functions that can be used when creating the neural network. Every cost function takes the target vector as its parameter, and returns a function valued only at X such that it may easily be differentiated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ab9e722",
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "\n",
    "def CostOLS(target):\n",
    "    \n",
    "    def func(X):\n",
    "        return (1.0 / target.shape[0]) * np.sum((target - X) ** 2)\n",
    "\n",
    "    return func\n",
    "\n",
    "\n",
    "def CostLogReg(target):\n",
    "\n",
    "    def func(X):\n",
    "        \n",
    "        return -(1.0 / target.shape[0]) * np.sum(\n",
    "            (target * np.log(X + 10e-10)) + ((1 - target) * np.log(1 - X + 10e-10))\n",
    "        )\n",
    "\n",
    "    return func\n",
    "\n",
    "\n",
    "def CostCrossEntropy(target):\n",
    "    \n",
    "    def func(X):\n",
    "        return -(1.0 / target.size) * np.sum(target * np.log(X + 10e-10))\n",
    "\n",
    "    return func"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eed4b4d",
   "metadata": {},
   "source": [
    "### Usage of cost functions\n",
    "Below we will provide a short example of how these cost function may be used to obtain results if you wish to test them out on your own using AutoGrad's automatics differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9b9d18a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Derivative of cost function CostCrossEntropy valued at a:\n",
      "[[-0.08333333]\n",
      " [-0.13333333]\n",
      " [-0.16666667]]\n"
     ]
    }
   ],
   "source": [
    "from autograd import grad\n",
    "\n",
    "target = np.array([[1, 2, 3]]).T\n",
    "a = np.array([[4, 5, 6]]).T\n",
    "\n",
    "cost_func = CostCrossEntropy\n",
    "cost_func_derivative = grad(cost_func(target))\n",
    "\n",
    "valued_at_a = cost_func_derivative(a)\n",
    "print(f\"Derivative of cost function {cost_func.__name__} valued at a:\\n{valued_at_a}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ba305e",
   "metadata": {},
   "source": [
    "## III. Activation functions\n",
    "Finally, before we look at the neural network, we will look at the activation functions which can be specified between the hidden layers and as the output function. Each function can be valued for any given vector or matrix X, and can be differentiated via derivate()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e282d670",
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd import elementwise_grad\n",
    "\n",
    "def identity(X):\n",
    "    return X\n",
    "\n",
    "\n",
    "def sigmoid(X):\n",
    "    try:\n",
    "        return 1.0 / (1 + np.exp(-X))\n",
    "    except FloatingPointError:\n",
    "        return np.where(X > np.zeros(X.shape), np.ones(X.shape), np.zeros(X.shape))\n",
    "\n",
    "\n",
    "def softmax(X):\n",
    "    X = X - np.max(X, axis=-1, keepdims=True)\n",
    "    delta = 10e-10\n",
    "    return np.exp(X) / (np.sum(np.exp(X), axis=-1, keepdims=True) + delta)\n",
    "\n",
    "\n",
    "def RELU(X):\n",
    "    return np.where(X > np.zeros(X.shape), X, np.zeros(X.shape))\n",
    "\n",
    "\n",
    "def LRELU(X):\n",
    "    delta = 10e-4\n",
    "    return np.where(X > np.zeros(X.shape), X, delta * X)\n",
    "\n",
    "\n",
    "def derivate(func):\n",
    "    if func.__name__ == \"RELU\":\n",
    "\n",
    "        def func(X):\n",
    "            return np.where(X > 0, 1, 0)\n",
    "\n",
    "        return func\n",
    "\n",
    "    elif func.__name__ == \"LRELU\":\n",
    "\n",
    "        def func(X):\n",
    "            delta = 10e-4\n",
    "            return np.where(X > 0, 1, delta)\n",
    "\n",
    "        return func\n",
    "\n",
    "    else:\n",
    "        return elementwise_grad(func)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1145f141",
   "metadata": {},
   "source": [
    "### Usage of activation functions\n",
    "Below is a short demonstration of how to use an activation function. The derivative of the activation function will be important when calculating the output delta term during backpropagation.  Note that derivate() can also be used for cost functions for a more generalized approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7f46ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input to activation function:\n",
      "[[4]\n",
      " [5]\n",
      " [6]]\n",
      "\n",
      "Output from sigmoid activation function:\n",
      "[[0.98201379]\n",
      " [0.99330715]\n",
      " [0.99752738]]\n",
      "\n",
      "Derivative of sigmoid activation function valued at z:\n",
      "[[0.19824029]\n",
      " [0.19721923]\n",
      " [0.19683648]]\n"
     ]
    }
   ],
   "source": [
    "z = np.array([[4, 5, 6]]).T\n",
    "print(f\"Input to activation function:\\n{z}\")\n",
    "\n",
    "act_func = sigmoid\n",
    "a = act_func(z)\n",
    "print(f\"\\nOutput from {act_func.__name__} activation function:\\n{a}\")\n",
    "\n",
    "act_func_derivative = derivate(act_func)\n",
    "valued_at_z = act_func_derivative(a)\n",
    "print(f\"\\nDerivative of {act_func.__name__} activation function valued at z:\\n{valued_at_z}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026e16e3",
   "metadata": {},
   "source": [
    "## IV. The Neural Network\n",
    "Now that we have gotten a good understanding of the implementation of some important components, we can take a look at an object oriented implementation of a feed forward neural network. The feed forward neural network has been implemented as a class named FFNN, which can be initiated as a regressor or classifier dependant on the choice of cost function. The FFNN can have any number of input nodes, hidden layers with any amount of hidden nodes, and any amount of output nodes meaning it can perform multiclass classification as well as binary classification and regression problems. Although there is a lot of code present, it makes for an easy to use and generalizeable interface for creating many types of neural networks as will be demonstrated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4196a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import autograd.numpy as np\n",
    "import sys\n",
    "import warnings\n",
    "from autograd import grad, elementwise_grad\n",
    "from random import random, seed\n",
    "from copy import deepcopy, copy\n",
    "from typing import Tuple, Callable\n",
    "from sklearn.utils import resample\n",
    "\n",
    "warnings.simplefilter(\"error\")\n",
    "\n",
    "\n",
    "class FFNN:\n",
    "    \"\"\"\n",
    "    Description:\n",
    "    ------------\n",
    "        Feed Forward Neural Network with interface enabling flexible design of a\n",
    "        nerual networks architecture and the specification of activation function\n",
    "        in the hidden layers and output layer respectively. This model can be used\n",
    "        for both regression and classification problems, depending on the output function.\n",
    "\n",
    "    Attributes:\n",
    "    ------------\n",
    "        I   dimensions (tuple[int]): A list of positive integers, which specifies the\n",
    "            number of nodes in each of the networks layers. The first integer in the array\n",
    "            defines the number of nodes in the input layer, the second integer defines number\n",
    "            of nodes in the first hidden layer and so on until the last number, which\n",
    "            specifies the number of nodes in the output layer.\n",
    "        II  hidden_func (Callable): The activation function for the hidden layers\n",
    "        III output_func (Callable): The activation function for the output layer\n",
    "        IV  cost_func (Callable): Our cost function\n",
    "        V   seed (int): Sets random seed, makes results reproducible\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dimensions: tuple[int],\n",
    "        hidden_func: Callable = sigmoid,\n",
    "        output_func: Callable = lambda x: x,\n",
    "        cost_func: Callable = CostOLS,\n",
    "        seed: int = None,\n",
    "    ):\n",
    "        self.dimensions = dimensions\n",
    "        self.hidden_func = hidden_func\n",
    "        self.output_func = output_func\n",
    "        self.cost_func = cost_func\n",
    "        self.seed = seed\n",
    "        self.weights = list()\n",
    "        self.schedulers_weight = list()\n",
    "        self.schedulers_bias = list()\n",
    "        self.a_matrices = list()\n",
    "        self.z_matrices = list()\n",
    "        self.classification = None\n",
    "\n",
    "        self.reset_weights()\n",
    "        self._set_classification()\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        X: np.ndarray,\n",
    "        t: np.ndarray,\n",
    "        scheduler: Scheduler,\n",
    "        batches: int = 1,\n",
    "        epochs: int = 100,\n",
    "        lam: float = 0,\n",
    "        X_val: np.ndarray = None,\n",
    "        t_val: np.ndarray = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        ------------\n",
    "            This function performs the training the neural network by performing the feedforward and backpropagation\n",
    "            algorithm to update the networks weights.\n",
    "\n",
    "        Parameters:\n",
    "        ------------\n",
    "            I    X (np.ndarray) : training data\n",
    "            II   t (np.ndarray) : target data\n",
    "            III  scheduler (Scheduler) : specified scheduler (algorithm for optimization of gradient descent)\n",
    "            IV   scheduler_args (list[int]) : list of all arguments necessary for scheduler\n",
    "\n",
    "        Optional Parameters:\n",
    "        ------------\n",
    "            V    batches (int) : number of batches the datasets are split into, default equal to 1\n",
    "            VI   epochs (int) : number of iterations used to train the network, default equal to 100\n",
    "            VII  lam (float) : regularization hyperparameter lambda\n",
    "            VIII X_val (np.ndarray) : validation set\n",
    "            IX   t_val (np.ndarray) : validation target set\n",
    "\n",
    "        Returns:\n",
    "        ------------\n",
    "            I   scores (dict) : A dictionary containing the performance metrics of the model.\n",
    "                The number of the metrics depends on the parameters passed to the fit-function.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # setup \n",
    "        if self.seed is not None:\n",
    "            np.random.seed(self.seed)\n",
    "\n",
    "        val_set = False\n",
    "        if X_val is not None and t_val is not None:\n",
    "            val_set = True\n",
    "\n",
    "        # creating arrays for score metrics\n",
    "        train_errors = np.empty(epochs)\n",
    "        train_errors.fill(np.nan)\n",
    "        val_errors = np.empty(epochs)\n",
    "        val_errors.fill(np.nan)\n",
    "\n",
    "        train_accs = np.empty(epochs)\n",
    "        train_accs.fill(np.nan)\n",
    "        val_accs = np.empty(epochs)\n",
    "        val_accs.fill(np.nan)\n",
    "\n",
    "        self.schedulers_weight = list()\n",
    "        self.schedulers_bias = list()\n",
    "\n",
    "        batch_size = X.shape[0] // batches\n",
    "\n",
    "        X, t = resample(X, t)\n",
    "\n",
    "        # this function returns a function valued only at X\n",
    "        cost_function_train = self.cost_func(t)\n",
    "        if val_set:\n",
    "            cost_function_val = self.cost_func(t_val)\n",
    "\n",
    "        # create schedulers for each weight matrix\n",
    "        for i in range(len(self.weights)):\n",
    "            self.schedulers_weight.append(copy(scheduler))\n",
    "            self.schedulers_bias.append(copy(scheduler))\n",
    "\n",
    "        print(f\"{scheduler.__class__.__name__}: Eta={scheduler.eta}, Lambda={lam}\")\n",
    "\n",
    "        try:\n",
    "            for e in range(epochs):\n",
    "                for i in range(batches):\n",
    "                    # allows for minibatch gradient descent\n",
    "                    if i == batches - 1:\n",
    "                        # If the for loop has reached the last batch, take all thats left\n",
    "                        X_batch = X[i * batch_size :, :]\n",
    "                        t_batch = t[i * batch_size :, :]\n",
    "                    else:\n",
    "                        X_batch = X[i * batch_size : (i + 1) * batch_size, :]\n",
    "                        t_batch = t[i * batch_size : (i + 1) * batch_size, :]\n",
    "\n",
    "                    self._feedforward(X_batch)\n",
    "                    self._backpropagate(X_batch, t_batch, lam)\n",
    "\n",
    "                # reset schedulers for each epoch (some schedulers pass in this call)\n",
    "                for scheduler in self.schedulers_weight:\n",
    "                    scheduler.reset()\n",
    "\n",
    "                for scheduler in self.schedulers_bias:\n",
    "                    scheduler.reset()\n",
    "\n",
    "                # computing performance metrics\n",
    "                pred_train = self.predict(X)\n",
    "                train_error = cost_function_train(pred_train)\n",
    "\n",
    "                train_errors[e] = train_error\n",
    "                if val_set:\n",
    "                    \n",
    "                    pred_val = self.predict(X_val)\n",
    "                    val_error = cost_function_val(pred_val)\n",
    "                    val_errors[e] = val_error\n",
    "\n",
    "                if self.classification:\n",
    "                    train_acc = self._accuracy(self.predict(X), t)\n",
    "                    train_accs[e] = train_acc\n",
    "                    if val_set:\n",
    "                        val_acc = self._accuracy(pred_val, t_val)\n",
    "                        val_accs[e] = val_acc\n",
    "\n",
    "                # printing progress bar\n",
    "                progression = e / epochs\n",
    "                print_length = self._progress_bar(\n",
    "                    progression,\n",
    "                    train_error=train_errors[e],\n",
    "                    train_acc=train_accs[e],\n",
    "                    val_error=val_errors[e],\n",
    "                    val_acc=val_accs[e],\n",
    "                )\n",
    "        except KeyboardInterrupt:\n",
    "            # allows for stopping training at any point and seeing the result\n",
    "            pass\n",
    "\n",
    "        # visualization of training progression (similiar to tensorflow progression bar)\n",
    "        sys.stdout.write(\"\\r\" + \" \" * print_length)\n",
    "        sys.stdout.flush()\n",
    "        self._progress_bar(\n",
    "            1,\n",
    "            train_error=train_errors[e],\n",
    "            train_acc=train_accs[e],\n",
    "            val_error=val_errors[e],\n",
    "            val_acc=val_accs[e],\n",
    "        )\n",
    "        sys.stdout.write(\"\")\n",
    "\n",
    "        # return performance metrics for the entire run\n",
    "        scores = dict()\n",
    "\n",
    "        scores[\"train_errors\"] = train_errors\n",
    "\n",
    "        if val_set:\n",
    "            scores[\"val_errors\"] = val_errors\n",
    "\n",
    "        if self.classification:\n",
    "            scores[\"train_accs\"] = train_accs\n",
    "\n",
    "            if val_set:\n",
    "                scores[\"val_accs\"] = val_accs\n",
    "\n",
    "        return scores\n",
    "\n",
    "    def predict(self, X: np.ndarray, *, threshold=0.5):\n",
    "        \"\"\"\n",
    "         Description:\n",
    "         ------------\n",
    "             Performs prediction after training of the network has been finished.\n",
    "\n",
    "         Parameters:\n",
    "        ------------\n",
    "             I   X (np.ndarray): The design matrix, with n rows of p features each\n",
    "\n",
    "         Optional Parameters:\n",
    "         ------------\n",
    "             II  threshold (float) : sets minimal value for a prediction to be predicted as the positive class\n",
    "                 in classification problems\n",
    "\n",
    "         Returns:\n",
    "         ------------\n",
    "             I   z (np.ndarray): A prediction vector (row) for each row in our design matrix\n",
    "                 This vector is thresholded if regression=False, meaning that classification results\n",
    "                 in a vector of 1s and 0s, while regressions in an array of decimal numbers\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        predict = self._feedforward(X)\n",
    "\n",
    "        if self.classification:\n",
    "            return np.where(predict > threshold, 1, 0)\n",
    "        else:\n",
    "            return predict\n",
    "\n",
    "    def reset_weights(self):\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        ------------\n",
    "            Resets/Reinitializes the weights in order to train the network for a new problem.\n",
    "\n",
    "        \"\"\"\n",
    "        if self.seed is not None:\n",
    "            np.random.seed(self.seed)\n",
    "\n",
    "        self.weights = list()\n",
    "        for i in range(len(self.dimensions) - 1):\n",
    "            weight_array = np.random.randn(\n",
    "                self.dimensions[i] + 1, self.dimensions[i + 1]\n",
    "            )\n",
    "            weight_array[0, :] = np.random.randn(self.dimensions[i + 1]) * 0.01\n",
    "\n",
    "            self.weights.append(weight_array)\n",
    "\n",
    "    def _feedforward(self, X: np.ndarray):\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        ------------\n",
    "            Calculates the activation of each layer starting at the input and ending at the output.\n",
    "            Each following activation is calculated from a weighted sum of each of the preceeding\n",
    "            activations (except in the case of the input layer).\n",
    "\n",
    "        Parameters:\n",
    "        ------------\n",
    "            I   X (np.ndarray): The design matrix, with n rows of p features each\n",
    "\n",
    "        Returns:\n",
    "        ------------\n",
    "            I   z (np.ndarray): A prediction vector (row) for each row in our design matrix\n",
    "        \"\"\"\n",
    "\n",
    "        # reset matrices\n",
    "        self.a_matrices = list()\n",
    "        self.z_matrices = list()\n",
    "\n",
    "        # if X is just a vector, make it into a matrix\n",
    "        if len(X.shape) == 1:\n",
    "            X = X.reshape((1, X.shape[0]))\n",
    "\n",
    "        # Add a coloumn of zeros as the first coloumn of the design matrix, in order\n",
    "        # to add bias to our data\n",
    "        bias = np.ones((X.shape[0], 1)) * 0.01\n",
    "        X = np.hstack([bias, X])\n",
    "\n",
    "        # a^0, the nodes in the input layer (one a^0 for each row in X - where the\n",
    "        # exponent indicates layer number).\n",
    "        a = X\n",
    "        self.a_matrices.append(a)\n",
    "        self.z_matrices.append(a)\n",
    "\n",
    "        # The feed forward algorithm\n",
    "        for i in range(len(self.weights)):\n",
    "            if i < len(self.weights) - 1:\n",
    "                z = a @ self.weights[i]\n",
    "                self.z_matrices.append(z)\n",
    "                a = self.hidden_func(z)\n",
    "                # bias column again added to the data here\n",
    "                bias = np.ones((a.shape[0], 1)) * 0.01\n",
    "                a = np.hstack([bias, a])\n",
    "                self.a_matrices.append(a)\n",
    "            else:\n",
    "                try:\n",
    "                    # a^L, the nodes in our output layers\n",
    "                    z = a @ self.weights[i]\n",
    "                    a = self.output_func(z)\n",
    "                    self.a_matrices.append(a)\n",
    "                    self.z_matrices.append(z)\n",
    "                except Exception as OverflowError:\n",
    "                    print(\n",
    "                        \"OverflowError in fit() in FFNN\\nHOW TO DEBUG ERROR: Consider lowering your learning rate or scheduler specific parameters such as momentum, or check if your input values need scaling\"\n",
    "                    )\n",
    "\n",
    "        # this will be a^L\n",
    "        return a\n",
    "\n",
    "    def _backpropagate(self, X: np.ndarray, t: np.ndarray, lam: float):\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        ------------\n",
    "            Performs the backpropagation algorithm. In other words, this method\n",
    "            calculates the gradient of all the layers starting at the\n",
    "            output layer, and moving from right to left accumulates the gradient until\n",
    "            the input layer is reached. Each layers respective weights are updated while\n",
    "            the algorithm propagates backwards from the output layer (auto-differentation in reverse mode).\n",
    "\n",
    "        Parameters:\n",
    "        ------------\n",
    "            I   X (np.ndarray): The design matrix, with n rows of p features each.\n",
    "            II  t (np.ndarray): The target vector, with n rows of p targets.\n",
    "            III lam (float32): regularization parameter used to punish the weights in case of overfitting\n",
    "\n",
    "        Returns:\n",
    "        ------------\n",
    "            No return value.\n",
    "\n",
    "        \"\"\"\n",
    "        out_derivative = derivate(self.output_func)\n",
    "        hidden_derivative = derivate(self.hidden_func)\n",
    "\n",
    "        for i in range(len(self.weights) - 1, -1, -1):\n",
    "            # delta terms for output\n",
    "            if i == len(self.weights) - 1:\n",
    "                # for multi-class classification\n",
    "                if (\n",
    "                    self.output_func.__name__ == \"softmax\"\n",
    "                ):\n",
    "                    delta_matrix = self.a_matrices[i + 1] - t\n",
    "                # for binary classification\n",
    "                else:\n",
    "                    cost_func_derivative = grad(self.cost_func(t))\n",
    "                    delta_matrix = out_derivative(\n",
    "                        self.z_matrices[i + 1]\n",
    "                    ) * cost_func_derivative(self.a_matrices[i + 1])\n",
    "\n",
    "            # delta terms for hidden layer\n",
    "            else:\n",
    "                delta_matrix = (\n",
    "                    self.weights[i + 1][1:, :] @ delta_matrix.T\n",
    "                ).T * hidden_derivative(self.z_matrices[i + 1])\n",
    "\n",
    "            # calculate gradient\n",
    "            gradient_weights = self.a_matrices[i][:, 1:].T @ delta_matrix\n",
    "            gradient_bias = np.sum(delta_matrix, axis=0).reshape(\n",
    "                1, delta_matrix.shape[1]\n",
    "            )\n",
    "\n",
    "            # regularization term\n",
    "            gradient_weights += self.weights[i][1:, :] * lam\n",
    "\n",
    "            # use scheduler\n",
    "            update_matrix = np.vstack(\n",
    "                [\n",
    "                    self.schedulers_bias[i].update_change(gradient_bias),\n",
    "                    self.schedulers_weight[i].update_change(gradient_weights),\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            # update weights and bias\n",
    "            self.weights[i] -= update_matrix\n",
    "\n",
    "    def _accuracy(self, prediction: np.ndarray, target: np.ndarray):\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        ------------\n",
    "            Calculates accuracy of given prediction to target\n",
    "\n",
    "        Parameters:\n",
    "        ------------\n",
    "            I   prediction (np.ndarray): vector of predicitons output network\n",
    "                (1s and 0s in case of classification, and real numbers in case of regression)\n",
    "            II  target (np.ndarray): vector of true values (What the network ideally should predict)\n",
    "\n",
    "        Returns:\n",
    "        ------------\n",
    "            A floating point number representing the percentage of correctly classified instances.\n",
    "        \"\"\"\n",
    "        assert prediction.size == target.size\n",
    "        return np.average((target == prediction))\n",
    "    \n",
    "    def _set_classification(self):\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        ------------\n",
    "            Decides if FFNN acts as classifier (True) og regressor (False),\n",
    "            sets self.classification during init()\n",
    "        \"\"\"\n",
    "        self.classification = False\n",
    "        if (\n",
    "            self.cost_func.__name__ == \"CostLogReg\"\n",
    "            or self.cost_func.__name__ == \"CostCrossEntropy\"\n",
    "        ):\n",
    "            self.classification = True\n",
    "\n",
    "    def _progress_bar(self, progression, **kwargs):\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        ------------\n",
    "            Displays progress of training\n",
    "        \"\"\"\n",
    "        print_length = 40\n",
    "        num_equals = int(progression * print_length)\n",
    "        num_not = print_length - num_equals\n",
    "        arrow = \">\" if num_equals > 0 else \"\"\n",
    "        bar = \"[\" + \"=\" * (num_equals - 1) + arrow + \"-\" * num_not + \"]\"\n",
    "        perc_print = self._format(progression * 100, decimals=5)\n",
    "        line = f\"  {bar} {perc_print}% \"\n",
    "\n",
    "        for key in kwargs:\n",
    "            if not np.isnan(kwargs[key]):\n",
    "                value = self._format(kwargs[key], decimals=4)\n",
    "                line += f\"| {key}: {value} \"\n",
    "        sys.stdout.write(\"\\r\" + line)\n",
    "        sys.stdout.flush()\n",
    "        return len(line)\n",
    "\n",
    "    def _format(self, value, decimals=4):\n",
    "        \"\"\"\n",
    "        Description:\n",
    "        ------------\n",
    "            Formats decimal numbers for progress bar\n",
    "        \"\"\"\n",
    "        if value > 0:\n",
    "            v = value\n",
    "        elif value < 0:\n",
    "            v = -10 * value\n",
    "        else:\n",
    "            v = 1\n",
    "        n = 1 + math.floor(math.log10(v))\n",
    "        if n >= decimals - 1:\n",
    "            return str(round(value))\n",
    "        return f\"{value:.{decimals-n-1}f}\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913fee68",
   "metadata": {},
   "source": [
    "## Usage of neural networks\n",
    "This section will show how to initiate and use various different neural networks.\n",
    "\n",
    "### Linear regression\n",
    " Before we make a model, we will quickly generate a dataset we can use for our linear regression problem as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87086a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def SkrankeFunction(x, y):\n",
    "    return np.ravel(0 + 1*x + 2*y + 3*x**2 + 4*x*y + 5*y**2)\n",
    "\n",
    "def create_X(x, y, n):\n",
    "    if len(x.shape) > 1:\n",
    "        x = np.ravel(x)\n",
    "        y = np.ravel(y)\n",
    "\n",
    "    N = len(x)\n",
    "    l = int((n + 1) * (n + 2) / 2)  # Number of elements in beta\n",
    "    X = np.ones((N, l))\n",
    "\n",
    "    for i in range(1, n + 1):\n",
    "        q = int((i) * (i + 1) / 2)\n",
    "        for k in range(i + 1):\n",
    "            X[:, q + k] = (x ** (i - k)) * (y**k)\n",
    "\n",
    "    return X\n",
    "\n",
    "step=0.2\n",
    "x = np.arange(0, 1, step)\n",
    "y = np.arange(0, 1, step)\n",
    "x, y = np.meshgrid(x, y)\n",
    "target = SkrankeFunction(x, y)\n",
    "target = target.reshape(target.shape[0], 1)\n",
    "\n",
    "poly_degree=3\n",
    "X = create_X(x, y, poly_degree)\n",
    "\n",
    "X_train, X_test, t_train, t_test = train_test_split(X, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "669caa92-e77d-4b20-a3b5-6ee3d7926450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(426, 30) (426, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, t_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d731194b",
   "metadata": {},
   "source": [
    "Now that we have our dataset ready for the regression, we can create our regressor. Note that with the seed parameter, we can make sure our results stay the same every time we run the neural network. For inititialization, we simply specify the dimensions (we wish the amount of input nodes to be equal to the datapoints, and the output to predict one value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93e202f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_nodes = X_train.shape[1]\n",
    "output_nodes = 1\n",
    "\n",
    "linear_regression = FFNN((input_nodes, output_nodes), output_func=identity, cost_func=CostOLS, seed=2023)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92473ad5",
   "metadata": {},
   "source": [
    "We then fit our model with our training data using the scheduler of our choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6b02b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constant: Eta=0.01, Lambda=0\n",
      "  [=======================================>] 100.0% | train_error: 2.11 "
     ]
    }
   ],
   "source": [
    "linear_regression.reset_weights() # reset weights such that previous runs or reruns don't affect the weights\n",
    "\n",
    "scheduler = Constant(eta=1e-2)\n",
    "scores = linear_regression.fit(X_train, t_train, scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3111a514",
   "metadata": {},
   "source": [
    "Due to the progress bar we can see the MSE (train_error) throughout the FFNN's training. Note that the fit() function has some optional parameters with defualt arguments. For example, the regularization hyperparameter can be left ignored if not needed, and equally the FFNN will by default run for 100 epochs. These can easily be changed, such as for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1cae319f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constant: Eta=0.01, Lambda=0.001\n",
      "  [=======================================>] 100.0% | train_error: 0.0312 "
     ]
    }
   ],
   "source": [
    "linear_regression.reset_weights() # reset weights such that previous runs or reruns don't affect the weights\n",
    "\n",
    "scores = linear_regression.fit(X_train, t_train, scheduler, lam=1e-3, epochs=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec76144",
   "metadata": {},
   "source": [
    "We see that given more epochs to train on, the regressor reaches a lower training MSE. If we want to use our test data, we simply pass it into our predict function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "59e11120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction of test data:\n",
      "[[8.11933242]\n",
      " [6.57621007]\n",
      " [3.13662589]\n",
      " [6.03188532]\n",
      " [2.22597104]\n",
      " [2.16715206]\n",
      " [3.6795256 ]]\n",
      "\n",
      "Target values of test data:\n",
      "[[7.64]\n",
      " [6.96]\n",
      " [3.  ]\n",
      " [6.12]\n",
      " [1.68]\n",
      " [2.24]\n",
      " [3.8 ]]\n",
      "\n",
      "test MSE:\n",
      "0.10305576953209486\n"
     ]
    }
   ],
   "source": [
    "pred_test = linear_regression.predict(X_test)\n",
    "\n",
    "print(f\"Prediction of test data:\\n{pred_test}\\n\")\n",
    "print(f\"Target values of test data:\\n{t_test}\\n\")\n",
    "print(f\"test MSE:\\n{CostOLS(t_test)(pred_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c2539d",
   "metadata": {},
   "source": [
    "### Binary classification\n",
    "Let us load in a binary classification dataset, here the Wisconsin Cancer data set, and follow a similar setup to the regression case. We will create a logistic regression classifier, and a feed forward neural network with 2 hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5aa72672",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "wisconsin = load_breast_cancer()\n",
    "X = wisconsin.data\n",
    "target = wisconsin.target\n",
    "target = target.reshape(target.shape[0], 1)\n",
    "\n",
    "X_train, X_val, t_train, t_val = train_test_split(X, target)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_val = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3a61e226",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_nodes = X_train.shape[1]\n",
    "output_nodes = 1\n",
    "\n",
    "logistic_regression = FFNN((input_nodes, output_nodes), output_func=sigmoid, cost_func=CostLogReg, seed=2023)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd912453",
   "metadata": {},
   "source": [
    "We will now make use of our validation data by passing it into our fit function as a keyword argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "634c8fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam: Eta=0.001, Lambda=0\n",
      "  [=======================================>] 100.0% | train_error: 14.0 | train_acc: 0.324 | val_error: 12.8 | val_acc: 0.385  "
     ]
    }
   ],
   "source": [
    "logistic_regression.reset_weights() # reset weights such that previous runs or reruns don't affect the weights\n",
    "\n",
    "scheduler = Adam(eta=1e-3, rho=0.9, rho2=0.999)\n",
    "scores = logistic_regression.fit(X_train, t_train, scheduler, epochs=500, X_val=X_val, t_val=t_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad60c785",
   "metadata": {},
   "source": [
    "Finally, we will create a neural network with 2 hidden layers with activation functions. We will also make use of the 'batches' keyword argument to run stochastic gradient descent by specifying a number of batches we wish to split our data into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8812fe13",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_nodes = X_train.shape[1]\n",
    "hidden_nodes1 = 100\n",
    "hidden_nodes2 = 30\n",
    "output_nodes = 1\n",
    "\n",
    "dims = (input_nodes, hidden_nodes1, hidden_nodes2, output_nodes)\n",
    "\n",
    "neural_network = FFNN(dims, hidden_func=RELU, output_func=sigmoid, cost_func=CostLogReg, seed=2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "32c6e297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam: Eta=0.0001, Lambda=0\n",
      "  [=======================================>] 100.0% | train_error: 0.195 | train_acc: 0.991 | val_error: 1.74 | val_acc: 0.916  "
     ]
    }
   ],
   "source": [
    "neural_network.reset_weights() # reset weights such that previous runs or reruns don't affect the weights\n",
    "\n",
    "scheduler = Adam(eta=1e-4, rho=0.9, rho2=0.999)\n",
    "scores = neural_network.fit(X_train, t_train, scheduler, batches=10, epochs=300, X_val=X_val, t_val=t_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf55a80",
   "metadata": {},
   "source": [
    "To plot the results, simply use the dictionary returned by the fit function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eb790ef5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(scores[\"train_accs\"], label=\"training\")\n",
    "plt.plot(scores[\"val_accs\"], label=\"validation\")\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807224cb",
   "metadata": {},
   "source": [
    "### Multiclass classification\n",
    "Finally, we will demonstrate the use case of multiclass classification using our FFNN with the famous MNIST dataset, which contain images of digits between the range of 0 to 9. The MNIST dataset has 64 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "70ed6a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "def onehot(target: np.ndarray):\n",
    "    onehot = np.zeros((target.size, target.max() + 1))\n",
    "    onehot[np.arange(target.size), target] = 1\n",
    "    return onehot\n",
    "\n",
    "digits = load_digits()\n",
    "\n",
    "X = digits.data\n",
    "target = digits.target\n",
    "target = onehot(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fed4ae3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_nodes = 64\n",
    "hidden_nodes1 = 100\n",
    "hidden_nodes2 = 30\n",
    "output_nodes = 10\n",
    "\n",
    "dims = (input_nodes, hidden_nodes1, hidden_nodes2, output_nodes)\n",
    "\n",
    "multiclass = FFNN(dims, hidden_func=LRELU, output_func=softmax, cost_func=CostCrossEntropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "15587373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam: Eta=0.0001, Lambda=0\n",
      "  [=======================================>] 100.0% | train_error: 0.178 | train_acc: 0.983 "
     ]
    }
   ],
   "source": [
    "multiclass.reset_weights() # reset weights such that previous runs or reruns don't affect the weights\n",
    "\n",
    "scheduler = Adam(eta=1e-4, rho=0.9, rho2=0.999)\n",
    "scores = multiclass.fit(X, target, scheduler, epochs=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5f6e3e",
   "metadata": {},
   "source": [
    "A quick note to finish on is that the different demonstrations that were made such as sending in the test data using predict for linear regression can as easily be done for a binary or multi class classification problem, and the same goes for vice versa for plotting different scores over epochs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
